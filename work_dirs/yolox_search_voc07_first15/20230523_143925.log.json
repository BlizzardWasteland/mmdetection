{"env_info": "sys.platform: linux\nPython: 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) [GCC 7.5.0]\nCUDA available: True\nGPU 0: NVIDIA TITAN X (Pascal)\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 11.5, V11.5.119\nGCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\nPyTorch: 1.7.0\nPyTorch compiling details: PyTorch built with:\n  - GCC 7.3\n  - C++ Version: 201402\n  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - CUDA Runtime 10.2\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37\n  - CuDNN 7.6.5\n  - Magma 2.5.2\n  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n\nTorchVision: 0.8.0\nOpenCV: 4.7.0\nMMCV: 1.7.1\nMMCV Compiler: GCC 7.3\nMMCV CUDA Compiler: 10.2\nMMDetection: 2.26.0+7a57807", "config": "checkpoint_config = dict(interval=1)\nlog_config = dict(interval=1, hooks=[dict(type='TextLoggerHook')])\ncustom_hooks = [\n    dict(type='SyncNormHook', num_last_epochs=15, interval=1, priority=48),\n    dict(\n        type='ExpMomentumEMAHook',\n        resume_from=None,\n        momentum=0.0001,\n        priority=49)\n]\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\nopencv_num_threads = 0\nmp_start_method = 'fork'\nauto_scale_lr = dict(enable=False, base_batch_size=64)\ncustom_imports = dict(\n    imports=['mmdet_custom.datasets', 'mmcv_custom.runner'],\n    allow_failed_imports=False)\nimg_scale = (640, 640)\nact_type = 'ReLU'\ndefault_widen_factor = 0.375\ndefault_deepen_factor = 0.33\nwiden_factor_range = [\n    0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666,\n    0.8333333333333334, 1\n]\ndeepen_factor_range = [0, 0.3333333333333333, 0.6666666666666666, 1]\nsearch_space = dict(\n    backbone_widen_factor_range=[\n        0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666,\n        0.8333333333333334, 1\n    ],\n    backbone_deepen_factor_range=[\n        0, 0.3333333333333333, 0.6666666666666666, 1\n    ],\n    neck_widen_factor_range=[1],\n    head_widen_factor_range=[\n        0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666,\n        0.8333333333333334, 1\n    ])\nresult = [4, 5, 5, 4, 0, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4]\narch = dict(\n    widen_factor_backbone_idx=[4, 5, 5, 4, 0],\n    deepen_factor_backbone_idx=[3, 3, 3, 2],\n    widen_factor_neck_idx=[0, 0, 0, 0, 0, 0, 0, 0],\n    widen_factor_neck_out_idx=4)\nwiden_factor_backbone = [0.3125, 0.375, 0.375, 0.3125, 0.0625]\ndeepen_factor_backbone = [0.33, 0.33, 0.33, 0.22]\nwiden_factor_neck = [1, 1, 1, 1, 1, 1, 1, 1]\nwiden_factor_head = [0.8333333333333334]\nin_channels = [96, 160, 64]\nhead_channels = 80\nmodel = dict(\n    type='SearchableYOLOX_KD',\n    is_kd=False,\n    bn_training_mode=False,\n    retraining=True,\n    search_space=dict(\n        backbone_widen_factor_range=[\n            0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666,\n            0.8333333333333334, 1\n        ],\n        backbone_deepen_factor_range=[\n            0, 0.3333333333333333, 0.6666666666666666, 1\n        ],\n        neck_widen_factor_range=[1],\n        head_widen_factor_range=[\n            0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666,\n            0.8333333333333334, 1\n        ]),\n    divisor=4,\n    input_size=(640, 640),\n    random_size_range=(10, 20),\n    random_size_interval=10,\n    backbone=dict(\n        type='SearchableCSPDarknetWOSPP',\n        deepen_factor=[0.33, 0.33, 0.33, 0.22],\n        widen_factor=[0.3125, 0.375, 0.375, 0.3125, 0.0625],\n        act_cfg=dict(type='ReLU')),\n    neck=dict(\n        type='SearchableYOLOXPAFPN',\n        in_channels=[96, 160, 64],\n        out_channels=80,\n        widen_factor=[1, 1, 1, 1, 1, 1, 1, 1],\n        num_csp_blocks=1,\n        act_cfg=dict(type='ReLU')),\n    bbox_head=dict(\n        type='SearchableYOLOXHead',\n        num_classes=15,\n        in_channels=80,\n        feat_channels=80,\n        act_cfg=dict(type='ReLU')),\n    train_cfg=dict(assigner=dict(type='SimOTAAssigner', center_radius=2.5)),\n    test_cfg=dict(score_thr=0.01, nms=dict(type='nms', iou_threshold=0.65)))\ndata_root = 'data/my_voc/'\ndataset_type = 'CocoDatasetContinual'\nCLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n           'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n           'person')\nimg_norm_cfg = dict(mean=[0, 0, 0], std=[255.0, 255.0, 255.0], to_rgb=True)\ntrain_pipeline = [\n    dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),\n    dict(\n        type='RandomAffine',\n        scaling_ratio_range=(0.5, 1.5),\n        border=(-320, -320)),\n    dict(type='YOLOXHSVRandomAug'),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Resize', img_scale=(640, 640), keep_ratio=True),\n    dict(\n        type='Pad',\n        pad_to_square=True,\n        pad_val=dict(img=(114.0, 114.0, 114.0))),\n    dict(\n        type='Normalize',\n        mean=[0, 0, 0],\n        std=[255.0, 255.0, 255.0],\n        to_rgb=True),\n    dict(type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n]\ntrain_dataset = dict(\n    type='MultiImageMixDataset',\n    dataset=dict(\n        type='CocoDatasetContinual',\n        ann_file=\n        'data/my_voc/annotations/voc07_trainval_sel_first_15_cats.json',\n        img_prefix='data/VOCdevkit/',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(type='LoadAnnotations', with_bbox=True)\n        ],\n        filter_empty_gt=False,\n        classes=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n                 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n                 'motorbike', 'person'),\n        previous_num_classes=0),\n    pipeline=[\n        dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),\n        dict(\n            type='RandomAffine',\n            scaling_ratio_range=(0.5, 1.5),\n            border=(-320, -320)),\n        dict(type='YOLOXHSVRandomAug'),\n        dict(type='RandomFlip', flip_ratio=0.5),\n        dict(type='Resize', img_scale=(640, 640), keep_ratio=True),\n        dict(\n            type='Pad',\n            pad_to_square=True,\n            pad_val=dict(img=(114.0, 114.0, 114.0))),\n        dict(\n            type='Normalize',\n            mean=[0, 0, 0],\n            std=[255.0, 255.0, 255.0],\n            to_rgb=True),\n        dict(\n            type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),\n        dict(type='DefaultFormatBundle'),\n        dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n    ])\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(416, 416),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(\n                type='Pad',\n                pad_to_square=True,\n                pad_val=dict(img=(114.0, 114.0, 114.0))),\n            dict(\n                type='Normalize',\n                mean=[0, 0, 0],\n                std=[255.0, 255.0, 255.0],\n                to_rgb=True),\n            dict(type='DefaultFormatBundle'),\n            dict(type='Collect', keys=['img'])\n        ])\n]\ndata = dict(\n    samples_per_gpu=8,\n    workers_per_gpu=4,\n    persistent_workers=True,\n    train=dict(\n        type='MultiImageMixDataset',\n        dataset=dict(\n            type='CocoDatasetContinual',\n            ann_file=\n            'data/my_voc/annotations/voc07_trainval_sel_first_15_cats.json',\n            img_prefix='data/VOCdevkit/',\n            pipeline=[\n                dict(type='LoadImageFromFile'),\n                dict(type='LoadAnnotations', with_bbox=True)\n            ],\n            filter_empty_gt=False,\n            classes=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n                     'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n                     'horse', 'motorbike', 'person'),\n            previous_num_classes=0),\n        pipeline=[\n            dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),\n            dict(\n                type='RandomAffine',\n                scaling_ratio_range=(0.5, 1.5),\n                border=(-320, -320)),\n            dict(type='YOLOXHSVRandomAug'),\n            dict(type='RandomFlip', flip_ratio=0.5),\n            dict(type='Resize', img_scale=(640, 640), keep_ratio=True),\n            dict(\n                type='Pad',\n                pad_to_square=True,\n                pad_val=dict(img=(114.0, 114.0, 114.0))),\n            dict(\n                type='Normalize',\n                mean=[0, 0, 0],\n                std=[255.0, 255.0, 255.0],\n                to_rgb=True),\n            dict(\n                type='FilterAnnotations',\n                min_gt_bbox_wh=(1, 1),\n                keep_empty=False),\n            dict(type='DefaultFormatBundle'),\n            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n        ]),\n    val=dict(\n        type='CocoDatasetContinual',\n        ann_file='data/my_voc/annotations/voc07_test_sel_first_15_cats.json',\n        img_prefix='data/VOCdevkit/',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(416, 416),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Pad',\n                        pad_to_square=True,\n                        pad_val=dict(img=(114.0, 114.0, 114.0))),\n                    dict(\n                        type='Normalize',\n                        mean=[0, 0, 0],\n                        std=[255.0, 255.0, 255.0],\n                        to_rgb=True),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ],\n        classes=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n                 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n                 'motorbike', 'person'),\n        previous_num_classes=0),\n    test=dict(\n        type='CocoDatasetContinual',\n        ann_file='data/my_voc/annotations/voc07_test_sel_first_15_cats.json',\n        img_prefix='data/VOCdevkit/',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(416, 416),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Pad',\n                        pad_to_square=True,\n                        pad_val=dict(img=(114.0, 114.0, 114.0))),\n                    dict(\n                        type='Normalize',\n                        mean=[0, 0, 0],\n                        std=[255.0, 255.0, 255.0],\n                        to_rgb=True),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ],\n        classes=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n                 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n                 'motorbike', 'person'),\n        previous_num_classes=0))\noptimizer = dict(\n    type='SGD',\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=0.0005,\n    nesterov=True,\n    paramwise_cfg=dict(norm_decay_mult=0.0, bias_decay_mult=0.0))\noptimizer_config = dict(grad_clip=None)\nmax_epochs = 300\nnum_last_epochs = 15\ninterval = 1\nlr_config = dict(\n    policy='YOLOX',\n    warmup='exp',\n    by_epoch=False,\n    warmup_by_epoch=True,\n    warmup_ratio=1,\n    warmup_iters=5,\n    num_last_epochs=15,\n    min_lr_ratio=0.05)\nrunner = dict(type='EpochBasedRunner', max_epochs=300)\nevaluation = dict(\n    save_best='auto', interval=10, dynamic_intervals=[(285, 1)], metric='mAP')\nwork_dir = './work_dirs/yolox_search_voc07_first15'\nauto_resume = False\ngpu_ids = [0]\n", "seed": 209652396, "exp_name": "yolox_search_voc07_first15.py", "hook_msgs": {}}
{"mode": "train", "epoch": 1, "iter": 1, "lr": 0.0, "memory": 2389, "data_time": 3.00938, "retrain_model.loss_cls": 0.99455, "retrain_model.loss_bbox": 4.77375, "retrain_model.loss_obj": 16.01946, "loss": 21.78776, "time": 3.41986}
{"mode": "train", "epoch": 1, "iter": 2, "lr": 0.0, "memory": 2402, "data_time": 0.01212, "retrain_model.loss_cls": 0.77446, "retrain_model.loss_bbox": 4.86594, "retrain_model.loss_obj": 20.61508, "loss": 26.25547, "time": 0.24541}
{"mode": "train", "epoch": 1, "iter": 3, "lr": 0.0, "memory": 2422, "data_time": 0.01194, "retrain_model.loss_cls": 1.29197, "retrain_model.loss_bbox": 4.63276, "retrain_model.loss_obj": 13.28748, "loss": 19.2122, "time": 0.23908}
{"mode": "train", "epoch": 1, "iter": 4, "lr": 0.0, "memory": 2422, "data_time": 0.00589, "retrain_model.loss_cls": 1.10687, "retrain_model.loss_bbox": 4.74423, "retrain_model.loss_obj": 22.72294, "loss": 28.57404, "time": 0.23295}
{"mode": "train", "epoch": 1, "iter": 5, "lr": 0.0, "memory": 2422, "data_time": 0.01128, "retrain_model.loss_cls": 1.26913, "retrain_model.loss_bbox": 4.66209, "retrain_model.loss_obj": 11.00857, "loss": 16.93979, "time": 0.2523}
{"mode": "train", "epoch": 1, "iter": 6, "lr": 0.0, "memory": 2422, "data_time": 0.06861, "retrain_model.loss_cls": 1.33539, "retrain_model.loss_bbox": 4.64084, "retrain_model.loss_obj": 31.58584, "loss": 37.56207, "time": 0.29704}
